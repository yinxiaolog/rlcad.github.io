<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user’s inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models’ vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points."
    />
    <meta name="keywords" content="Computer-Aided Design Models, Multimodal Large Language Models, Multimodality Data" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation
    </title>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>


    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚙️</text></svg>">

    <link rel="stylesheet" href="static/css/bootstrap.min.css">
    <link rel="stylesheet" href="static/css/app.css"/>
    <script src="static/js/app.js"></script>
    <link rel="stylesheet" href="static/css/dics.min.css"/>
    <script src="static/js/dics.min.js"></script>
    <script src="static/js/video_comparison.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
    <script src="static/js/index.js"></script>
    <script src="static/js/zotero-meta.js"></script>
    <script>
      $(document).ready(function () {
        const citations = [
          {
            name: "citation_title",
            content:
              "RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation",
          },
          {
            name: "citation_author",
            content: "Xu, Jingwei",
          },
          {
            name: "citation_author",
            content: "Wang, Chenyu",
          },
          {
            name: "citation_author",
            content: "Zhao, Zibo",
          },
          {
            name: "citation_author",
            content: "Liu, Wen",
          },
          {
            name: "citation_author",
            content: "Ma, Yi",
          },
          {
            name: "citation_author",
            content: "Gao, Shenghua",
          },
          {
            name: "citation_date",
            content: "2024/11/7",
          },
          {
            name: "citation_online_date",
            content: "2024/11/7",
          },
          {
            name: "citation_pdf_url",
            content: "https://arxiv.org/pdf/2411.04954",
          },
          {
            name: "citation_arxiv_id",
            content: "2411.04954",
          },
          {
            name: "citation_abstract",
            content:
              "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user’s inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models’ vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points.",
          },
        ];
        createCitationMeta(citations);
      });
    </script>
  </head>

  <style>
    button {
      padding: 0.5em 0.75em;
      margin: 0.4em 0.4em;
      /* min-width: 18ch; */
      /* max-width: 18ch; */
      text-align: left;
      background-color: #ddd;
      color: #333;
      border-radius: 5px;
      border: none;
      cursor: pointer;
    
      @media screen and (-ms-high-contrast: active) {
        border: 2px solid currentcolor;
      }
    }
  </style>
  <body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title" style="font-size: 36px">
                  RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block" style="font-size: 22px">
                  <span>Xiaolong Yin</span><sup>1*</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Xingyu Lu</span><sup>1*</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span">Jiahang Shen</span><sup>1</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Jingzhe Ni</span ><sup>1</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Hailong Li</span><sup>2</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Ruofeng Tong</span><sup>1</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Min Tang</span><sup>1</sup>,
                </span>
                <span class="author-block" style="font-size: 22px">
                  <span>Peng Du</span><sup>1†</sup>,
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block" style="font-size: 18px"
                  ><sup>1</sup>Zhejiang University
                </span>
                <span class="author-block" style="font-size: 18px"
                  ><sup>2</sup>Shenzhen Poisson Software Co., Ltd.
                </span>
              </div>
              <div class="is-size-5 publication-authors">
                (* denotes equal contribution, † denotes the corresponding author)
              </div>
              <div class="column has-text-centered">
                <div class="link-block">
                  <!-- PDF Link. -->
                  <!-- TODO -->
                  <!-- <span class="link-block">
                    <a
                      href=""
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper(Coming soon)</span>
                    </a>
                  </span> -->
                  <span class="link-block">
                    <a
                      href="https://www.sciencedirect.com/science/article/abs/pii/S0010448525001885"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                  <!-- Code Link. -->
                  <!-- TODO -->
                  <span class="link-block">
                    <a
                      href=""
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code(Coming soon)</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href=""
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset(Coming soon)</span>
                    </a>
                  </span>
                  <!-- Dataset Link. -->
                  <!-- <span class="link-block">
                <a href="https://mega.nz/folder/jdhDnTqL#Ija678SU2Va_JJOiwqmdEg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
                </div>
              </div>

              <!-- <div style="font-size: 10px"> -->
              <!--   <a href="https://www.zotero.org/" -->
              <!--     ><b style="color: #a6212c">Z</b>otero Connector</a -->
              <!--   > -->
              <!--   friendly -->
              <!-- </div> -->
            </div>
          </div>
        </div>
      </div>
    </section>


    <div class="columns is-centered has-text-centered" id="video">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-2">Demo</h2> -->
        <!-- <video class="video" width=100% id="original" loop playsinline autoplay muted src="./static/videos/demo.mp4" onplay="resizeAndPlay(this)"></video> --> 

        <video controls>
          <source src="static/videos/demo0.mp4" type="video/mp4">
          Your browser does not support the video tag.
      </video>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" id="video">
      <div class="column is-four-fifths">
          <h2 class="title is-2">Face-Extrusion and Revolution Operation</h2>
      </div>
    </div>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="pic/battery_actions_b_01.png"></img>
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            Battery modeling process using the gym interface. The leftmost image shows face IDs on the surfaces. The right sequence illustrates four extrusion/revolution operations with Boolean operations applied iteratively to generate the final geometry.</h2>
        </div>
      </div>
    </section>


    <div class="columns is-centered has-text-centered" id="video">
      <div class="column is-four-fifths">
          <h2 class="title is-2">Network Architecture</h2>
      </div>
    </div>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="pic/figure2_v15_01.png" alt="CAD-MLLM" />
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            The training pipeline is composed of two stages. In the first stage, a contrastive learning approach is employed to pre-train the UV-Net network, aiming to derive an encoder model that can effectively characterize the B-Rep of the CAD model. During the second stage, a reinforcement learning approach is employed to generate the command sequence. We first utilize the tunable UV-Net model to extract the B-Rep embedding of the CAD model, which is then integrated with the feature vector of the historical modeling action sequence. Subsequently, the Actor-Critic network predicts the action distribution and value. The predicted action is transmitted to RLCADGym for execution, yielding the next-stage observation. The neural reward and geometric reward are designed to update the policy network.
          </h2>
        </div>
      </div>
    </section>

    <div class="columns is-centered has-text-centered" id="video">
      <div class="column is-four-fifths">
          <h2 class="title is-2">Our Dataset(Omni-CAD)</h2>
      </div>
    </div>

    <!-- <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/dataset_vis.png" alt="CAD-MLLM" />
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            <b>Qualitative Comparison:</b> We exclude the CAD models’ IDs that have been included in the DeepCAD dataset for visualization. The extension part of our dataset contains more complex and realistic models with more details.
          </h2>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/dataset_multimodal.png" alt="CAD-MLLM" />
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            Example of the conditioned multimodality data and the corresponding ground truth CAD models.
          </h2>
        </div>
      </div>
    </section> -->

    <div class="columns is-centered has-text-centered" id="video">
      <div class="column is-four-fifths">
          <h2 class="title is-2">Result Comparison of Reconstruction</h2>
          (Please view our paper for more results)
      </div>
    </div>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="pic/render-models_v4_01.png" alt="CAD-MLLM" />
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            Comparison of generation results with Fusion 360 Gallery. It shows our method generates higher-quality results in terms of both completeness and detail.
          </h2>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <div style="text-align: center;">
          <img src="pic/compare-eth_01.png" width="100%" alt="CAD-MLLM"/>
          </div>
          <h2 class="subtitle has-text-centered" style="font-size: 16px">
            Comparison of generation results with cadrille and CAD-Recode. It shows our method generates higher-quality results in terms of both completeness and detail.
          </h2>
        </div>
      </div>
    </section>
   
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <p>If you find this work useful, please cite:</p>
        <pre><code>
@article{yin2025rlcad,
  title={Rlcad: Reinforcement learning training gym for revolution involved cad command sequence generation},
  author={Yin, Xiaolong and Lu, Xingyu and Shen, Jiahang and Ni, Jingzhe and Li, Hailong and Tong, Ruofeng and Tang, Min and Du, Peng},
  journal={Computer-Aided Design},
  pages={104027},
  year={2025},
  publisher={Elsevier}
}
        </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <!-- TODO -->
          <a class="icon-link" href="https://arxiv.org/abs/2411.04954">
            <i class="fas fa-file-pdf"></i>
          </a>
            <!-- TODO -->
          <a
            class="icon-link"
            href="https://github.com/CAD-MLLM/CAD-MLLM"
            class="external-link"
            disabled
          >
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </footer>

    <script>
      bulmaCarousel.attach("#geo-carousel", {
        slidesToScroll: 1,
        slidesToShow: 1,
        infinite: true,
      });
      bulmaCarousel.attach("#novel-carousel", {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: true,
      });
      bulmaCarousel.attach("#edit-carousel", {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: true,
      });
    </script>
  </body>
</html>
